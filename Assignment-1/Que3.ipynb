{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Que3  - Answer\n",
    "\n",
    "### Proof for factor of two between optimal when centers are data points v/s when centers can be any point -\n",
    "\n",
    "Suppose, we have some data set say $S$ and we want to find k centers for clustering it. \n",
    "\n",
    "Now, let $a = \\{ a_{1},a_{2},a_{3}...,a_{k} \\}$ be the optimal k-centers for the given data set $S$. Then, the cost is -\n",
    "\n",
    "$cost(a) = \\sum_{x} min_{i}d(x, a_{i})$, where $a_{i} \\in R^{d}$, where $d$ = dimension of data set (here we do not have any restriction on the centers, i.e. $a_{i}$ can be any point) \n",
    "\n",
    "Where, $d(x,a_{i})$ is the L2 (without square) Euclidean distance between vectors $x$ and $a_{i}$.\n",
    "\n",
    "This is the optimal cost for clustering for $S$ using L2 Euclidean distance.\n",
    "\n",
    "Now, let $b = \\{ b_{1},b_{2},b_{3}...,b_{k} \\}$ be a set such that $b_{i} \\in$ $S$ $\\forall i$ and $b_{i}$ is the closest data point to $a_{i}$ $\\forall i$\n",
    "\n",
    "then, cost with these centers is -\n",
    "\n",
    "$cost(b) = \\sum_{x} min_{i}d(x, b_{i})$, where\n",
    "\n",
    "Trivially $cost(a) \\le cost(b)$ since $cost(a)$ is the optimal and in worst case it can always use $a_{i} = b_{i}$ $\\forall i$ \n",
    "\n",
    "Now, for any point $x \\in S$ and for a fixed $i$, we have\n",
    "\n",
    "$d(x, b_{i}) \\le d(x, a_{i}) + d(a_{i}, b_{i}) $ by triangle inequality\n",
    "\n",
    "But, $d(a_{i}, b_{i}) \\le d(x, a_{i})$ since, $b_{i}$ is the closest point to $a_{i}$ that lies in $S$\n",
    "\n",
    "=> $d(x, b_{i}) \\le d(x, a_{i}) + d(x, a_{i}) $\n",
    "\n",
    "=> $d(x, b_{i}) \\le 2 \\times d(x, a_{i})  $\n",
    "\n",
    "=> $ min_{i} d(x, b_{i}) \\le  min_{i} (2 \\times d(x, a_{i}) )$\n",
    "\n",
    "=> $ \\sum_{x} min_{i} d(x, b_{i}) \\le  \\sum_{x} min_{i} (2 \\times d(x, a_{i}) )$\n",
    "\n",
    "=> $ \\sum_{x} min_{i} d(x, b_{i}) \\le  2 \\times  \\sum_{x} min_{i} (d(x, a_{i}) )$\n",
    "\n",
    "=> $ cost(b) \\le  2 \\times  cost(a)$.\n",
    "\n",
    "Hence, this cost (i.e. $cost(b)$, when centers are restricted to be data points only) is bounded by a factor of $2$ with the optimal cost (i.e. $cost(a)$, when center can be any point)\n",
    "\n",
    "But $cost(b)$ may not be the optimal cost when centers are are restricted to be among data points, let $cost(o)$ be that optimal cost.\n",
    "\n",
    "Then, $cost(o) \\le cost(b)$\n",
    "\n",
    "Hence, from the previous equation we get,\n",
    "\n",
    "$cost(o) \\le cost(b) \\le 2 \\times cost(a)$\n",
    "\n",
    "=> $cost(o) \\le 2 \\times cost(a)$\n",
    "\n",
    "Thus, the optimal cost when centers have to be among data points is bounded by a factor of $2$ with the optimal cost when centers could be any point.\n",
    "\n",
    "Hence, proved.\n",
    "\n",
    "### Variant of Lloydâ€™s algorithm for Euclidean k-median -\n",
    "\n",
    "- Choose $k$ centers (points) from the given set of data points (arbitrarily or by using the initialization of kmeans++)\n",
    "- For each point in our data set assign it a center (from the set of $k$ centers that we have currently) such that the center is closest to the data point based on Euclidean distance.\n",
    "- Now, let us call a cluster as a set of those data points which have the same center. There will be k such clusters.\n",
    "- Find medians for each of these k clusters. This can be done by finding median in each of the dimensions.\n",
    "- Replace k center with these new medians.\n",
    "- Go to step 2 and repeat until a criteria is fulfilled.\n",
    "- At the end, for each of the k centers obtained we choose a point in the data set that is closest to that center (based on Euclidean distance). Let, these new centers (in the data points) be the final clustering points.\n",
    "\n",
    "The criteria for stopping could be that we stop when k-median cost is not decreasing or is decreasing by a very small value or the k-medians are not changing very much. \n",
    "\n",
    "At each step, we use dimension-wise median to find the new median which may not lie in the data points. But, in the end we choose closest points (in the data points) to the k centers obtained so that our final center points are part of the data points.\n",
    "\n",
    "### Is cost always decreasing -\n",
    "\n",
    "Yes, at each step the cost will decrease since we are updating centers using median (dimension-wise). However at last when we choose centers among data points, the cost may increase."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
